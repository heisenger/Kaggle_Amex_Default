{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0dfbeeb-12e3-48bc-874c-0a8e4c4dfbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "# import cupy\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from DataPipeline import agglomeration_function\n",
    "from DataPipeline import null_filtering\n",
    "from DataPipeline import pre_process_df\n",
    "\n",
    "import optuna\n",
    "\n",
    "from EvaluationMetrics import amex_metric_mod\n",
    "from EvaluationMetrics import top_four_percent\n",
    "from EvaluationMetrics import gini_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4139ed1-e8b3-4719-92d5-62242ca1c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce8c777-d934-4a7d-9a79-fb904785ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Parameters:\n",
    "    compute_train_df = True # if True, we will compute a new train_df, otherwise we read from drive\n",
    "    compute_test_df = True # if True, we will compute a new test_df, otherwise we read from drive\n",
    "    fast_training = False # if True, we train on only 10% of the rows\n",
    "    train_hard_samples = False\n",
    "\n",
    "    if fast_training:\n",
    "        training_rounds = 999\n",
    "    else:\n",
    "        training_rounds = 30000\n",
    "\n",
    "train_df = pd.read_parquet('Processed_Data/cleaned_train.parquet')\n",
    "train_df = pre_process_df(train_df)\n",
    "\n",
    "if Train_Parameters.fast_training:\n",
    "    train_df = train_df.sample(frac=0.01, ignore_index=True)\n",
    "    \n",
    "null_threshold = 0.99\n",
    "null_columns = null_filtering(train_df.iloc[:, 1:-1], null_threshold=null_threshold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da88c6fe-6d0e-407a-b769-6295cb4978d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key parameters\n",
    "categorical_columns = [\"D_63\", \n",
    "                       \"D_64\", \"D_66\", \n",
    "                       \"D_68\", \"B_30\", \n",
    "                       \"B_38\", \"D_114\", \n",
    "                       \"D_116\", \"D_117\", \n",
    "                       \"D_120\", \"D_126\"]  \n",
    "\n",
    "class Model_Parameters:\n",
    "    irrelevant_columns = [\"customer_ID\", \"target\"]\n",
    "    other_columns = ['S_2'] #['S_2']\n",
    "    categorical_columns = [\"D_63\", \n",
    "                           \"D_64\", \"D_66\", \n",
    "                           \"D_68\", \"B_30\", \n",
    "                           \"B_38\", \"D_114\", \n",
    "                           \"D_116\", \"D_117\", \n",
    "                           \"D_120\", \"D_126\"]  \n",
    "    train_test_delta_columns = ['R_1', 'D_59', 'S_11', 'B_29', 'S_9', 'D_45']#, \n",
    "                                # 'D_121', 'S_27', 'S_24', 'S_22', 'D_115', 'D_118', \n",
    "                                # 'D_119', 'S_13', 'D_47', 'D_55', 'S_3', 'B_6', 'D_61', \n",
    "                                # 'P_3', 'D_52', 'B_9', 'P_4', 'S_7', 'B_12', 'D_62', \n",
    "                                # 'D_42', 'B_13', 'D_43', 'P_2', 'B_28', 'B_38', 'D_46', \n",
    "                                # 'B_25', 'B_14', 'B_3', 'S_5', 'B_2', 'B_40', 'D_48', 'D_60'] \n",
    "                                # 'B_10', 'S_12', 'B_37', 'B_1', 'D_69', 'B_5', 'B_7', 'B_11', 'B_18', \n",
    "                                # 'D_71', 'D_39', 'B_23', 'S_8', 'B_17', 'S_17', 'D_77', 'D_142', 'S_25', 'D_141', 'B_15', 'D_58', \n",
    "                                # 'R_27', 'S_23', 'S_26', 'D_105', 'D_50', 'D_124', 'B_4', 'D_102', 'D_133', 'D_56',\n",
    "                                # 'D_104', 'B_21', 'S_16', 'D_144', 'R_6', 'B_36', 'S_19', 'B_26', 'B_24', 'B_27', 'D_128', 'B_16', 'B_8', 'D_53', \n",
    "                                # 'D_112', 'B_19', 'D_41', 'D_132', 'D_130', 'D_74', 'D_68', 'D_131', 'D_75', 'B_20', 'D_76', 'D_44', 'D_122', 'D_49', 'D_113', \n",
    "                                # 'D_117', 'D_145', 'S_15', 'D_134', 'D_106', 'D_65', 'D_63', 'R_3', 'R_7', 'D_54', 'D_70', 'R_12', 'B_42', 'R_14', 'D_73', 'D_80', \n",
    "                                # 'R_26', 'D_78', 'D_72', 'B_39', 'D_84', 'D_107', 'D_110', 'D_64', 'R_16', 'D_82', 'R_5', 'R_9', 'D_81', 'R_8', 'B_22', 'R_20']#, \n",
    "                                # # 'D_103', 'R_10', 'D_79', 'D_136', 'D_139', 'R_11', 'B_41', 'D_51', 'D_91', 'D_143', 'D_83', 'D_89', 'R_17', 'B_30',\n",
    "                                # # 'R_13', 'D_125', 'D_138', 'D_123', 'D_135', 'D_140', 'D_129', 'D_66', 'D_88', 'R_4', 'B_33', 'R_2', 'D_92', 'D_111' \n",
    "                                # # 'S_18', 'D_108', 'R_21', 'D_114', 'R_24', 'D_86', 'D_137', 'D_116', 'D_126', 'D_109', 'D_120', 'D_96', 'B_31']\n",
    "    \n",
    "    train_test_delta_columns = [i for i in train_test_delta_columns if i not in categorical_columns]\n",
    "    \n",
    "    #['R_1', 'D_59', 'S_11', 'B_29', 'S_9'] #['R_1', 'D_59', 'S_11', 'B_29', 'S_9'] # columns with different distributions between train and test\n",
    "    \n",
    "    ignored_columns = train_test_delta_columns + list(null_columns.values) + other_columns #+ categorical_columns \n",
    "    valid_size = 0.2\n",
    "    SEED = 10\n",
    "    FOLDS = 5 #5\n",
    "    VER = 5 #\"HARD_Samples_1\" 'hard_indicator_1'\n",
    "    \n",
    "num_features = [col for col in train_df.columns if ((col not in Model_Parameters.ignored_columns) \n",
    "                                                    and (col not in Model_Parameters.categorical_columns) \n",
    "                                                    and (col not in Model_Parameters.irrelevant_columns))]\n",
    "cat_features = [col for col in train_df.columns if col in Model_Parameters.categorical_columns]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c3c557e-2968-4f80-8617-612252fa4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_parms = { \n",
    "    'max_depth':6, #5, \n",
    "    'learning_rate':0.013, #0.05, \n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.6, \n",
    "    'eval_metric':'logloss',\n",
    "    'objective':'binary:logistic',\n",
    "    'tree_method':'gpu_hist',\n",
    "    'predictor':'gpu_predictor',\n",
    "    'random_state':Model_Parameters.SEED,\n",
    "    'scale_pos_weight':3 #3 #use 10 when doing hard samples mining\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d704ac69-2d21-494d-97bd-7b55d2d17224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (5531451, 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/Desktop/Kaggle/amex_default_prediction/DataPipeline.py:47: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "/home/julian/Desktop/Kaggle/amex_default_prediction/DataPipeline.py:48: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "/home/julian/Desktop/Kaggle/amex_default_prediction/DataPipeline.py:55: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "/home/julian/Desktop/Kaggle/amex_default_prediction/DataPipeline.py:56: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (458913, 2764)\n"
     ]
    }
   ],
   "source": [
    "# Create Train_df\n",
    "if Train_Parameters.compute_train_df:\n",
    "    train = train_df.loc[:, ~train_df.columns.isin(Model_Parameters.ignored_columns)]\n",
    "\n",
    "    train = agglomeration_function(train, num_features=num_features, cat_features=cat_features, apply_pca=False)\n",
    "    customer_ID_cols = train_df.groupby('customer_ID')['customer_ID'].tail(1).reset_index(drop=True)\n",
    "\n",
    "    train = pd.concat([customer_ID_cols, train], axis=1)\n",
    "    train.to_csv('Processed_Data/train_df_cleaned.csv', index=False)\n",
    "    \n",
    "\n",
    "else:\n",
    "    if Train_Parameters.fast_training:\n",
    "        train = pd.read_csv('Processed_Data/train_df_cleaned.csv', nrows=10000)\n",
    "        train = train.sample(frac=0.1, ignore_index=True)\n",
    "    else:\n",
    "        train = pd.read_csv('Processed_Data/train_df_cleaned.csv')\n",
    "        \n",
    "train_dtypes = train.iloc[:,1:-1].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3b36a33-e2e4-4480-9abd-a3bf3776b809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold\t Model_Parameters\t StratifiedKFold\t Train_Parameters\t agglomeration_function\t amex_metric_mod\t cat_features\t categorical_columns\t customer_ID_cols\t \n",
      "gc\t gini_metric\t lgb\t np\t null_columns\t null_filtering\t null_threshold\t num_features\t optuna\t \n",
      "pd\t permutation_importance\t plot_confusion_matrix\t plt\t pre_process_df\t top_four_percent\t train\t train_df\t train_dtypes\t \n",
      "train_test_split\t xgb\t xgb_parms\t \n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "829fff6f-040b-4852-9126-6a7d7ff9adfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "         3 function calls in 0.000 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    0.000    0.000 <string>:1(<module>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun 20+20\n",
    "5+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925ee376-9785-4641-b156-4b68c929194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OOF Analysis\n",
    "\n",
    "# oof_xgb = pd.read_csv(f'oof_predictions/oof_xgb_v{Model_Parameters.VER}.csv')\n",
    "# # oof_xgb[oof_xgb.target==1].hist()\n",
    "# # oof_xgb[oof_xgb.target==0].hist()\n",
    "# print('top 4% ', top_four_percent(oof_xgb.target.values, oof_xgb.oof_pred.values), '/n',\n",
    "# 'gini metric ', gini_metric(oof_xgb.target.values, oof_xgb.oof_pred.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dbdae2a-46e3-4df8-948d-aa6b9f632f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create hard_df indicator\n",
    "# ##############################\n",
    "# if Train_Parameters.train_hard_samples:\n",
    "\n",
    "#     hard_df = oof_xgb[\n",
    "#         ((oof_xgb.target==1).values & (oof_xgb.oof_pred<=0.5).values) | \n",
    "#         ((oof_xgb.target==0).values & (oof_xgb.oof_pred>=0.5).values)  \n",
    "#     ] \n",
    "#     hard_df.head()\n",
    "#     hard_df['hard_indicator'] = 1\n",
    "\n",
    "#     train = train.merge(hard_df[[\"customer_ID\",\"hard_indicator\"]], how='left', left_on='customer_ID', right_on='customer_ID')\n",
    "#     train.hard_indicator.fillna(0, inplace=True)\n",
    "\n",
    "#     train.drop(columns='target', inplace=True)\n",
    "#     train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7213f7fb-6f5c-476e-b4aa-308e3197b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Main training routine\n",
    "#####\n",
    "\n",
    "def train_xgb_model(X_valid, Y_valid, X_train, Y_train, params=None, num_rounds=1000, existing_model=None):\n",
    "    dtrain = xgb.DMatrix(data=X_train, label=Y_train, enable_categorical=True)\n",
    "    dvalid = xgb.DMatrix(data=X_valid, label=Y_valid, enable_categorical=True)\n",
    "\n",
    "    if existing_model == None:\n",
    "        model = xgb.train(params, \n",
    "                dtrain=dtrain,\n",
    "                evals=[(dtrain,'train'),(dvalid,'valid')],\n",
    "                num_boost_round=num_rounds,\n",
    "                early_stopping_rounds=1000,\n",
    "                verbose_eval=100)\n",
    "    else:\n",
    "        model = xgb.train(params, \n",
    "                dtrain=dtrain,\n",
    "                evals=[(dtrain,'train'),(dvalid,'valid')],\n",
    "                num_boost_round=num_rounds,\n",
    "                early_stopping_rounds=1000,\n",
    "                verbose_eval=100,\n",
    "                xgb_model=existing_model) \n",
    "\n",
    "    oof_pred = model.predict(dvalid)\n",
    "        \n",
    "    return model, oof_pred\n",
    "\n",
    "def train_lgb_model(X_valid, Y_valid, X_train, Y_train, params=None, num_rounds=1000):\n",
    "    dtrain = lgb.Dataset(X_train, Y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, Y_valid, reference=dtrain)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting':'gbdt',\n",
    "        'seed': 42,\n",
    "        'num_leaves': 150, #100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 10, #2\n",
    "        'min_data_in_leaf': 2000,#40\n",
    "        'scale_pos_weight' : 0.1\n",
    "       \n",
    "    }\n",
    "    \n",
    "    model = lgb.train(params, \n",
    "            dtrain,\n",
    "            valid_sets=[dtrain, dvalid],\n",
    "            num_boost_round=num_rounds,\n",
    "            early_stopping_rounds=2000,\n",
    "            verbose_eval=100)\n",
    "\n",
    "    oof_pred = model.predict(X_valid)\n",
    "        \n",
    "    return model, oof_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e00cd67-0c05-437b-a517-886a1c51b05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "### Train size 367130 Valid size 91783\n",
      "### Train shape (458913, 2253)\n",
      "### Training with 100% fold data...\n",
      "#########################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.449621\tvalid_1's binary_logloss: 0.450597\n",
      "[200]\ttraining's binary_logloss: 0.422049\tvalid_1's binary_logloss: 0.424206\n",
      "[300]\ttraining's binary_logloss: 0.414199\tvalid_1's binary_logloss: 0.417323\n",
      "[400]\ttraining's binary_logloss: 0.409661\tvalid_1's binary_logloss: 0.413661\n",
      "[500]\ttraining's binary_logloss: 0.40533\tvalid_1's binary_logloss: 0.410108\n",
      "[600]\ttraining's binary_logloss: 0.40056\tvalid_1's binary_logloss: 0.406229\n",
      "[700]\ttraining's binary_logloss: 0.396132\tvalid_1's binary_logloss: 0.402637\n",
      "[800]\ttraining's binary_logloss: 0.391834\tvalid_1's binary_logloss: 0.399319\n",
      "[900]\ttraining's binary_logloss: 0.387791\tvalid_1's binary_logloss: 0.396318\n",
      "[1000]\ttraining's binary_logloss: 0.383842\tvalid_1's binary_logloss: 0.393368\n",
      "[1100]\ttraining's binary_logloss: 0.380668\tvalid_1's binary_logloss: 0.3913\n",
      "[1200]\ttraining's binary_logloss: 0.377526\tvalid_1's binary_logloss: 0.389292\n",
      "[1300]\ttraining's binary_logloss: 0.374536\tvalid_1's binary_logloss: 0.387419\n",
      "[1400]\ttraining's binary_logloss: 0.371379\tvalid_1's binary_logloss: 0.385354\n",
      "[1500]\ttraining's binary_logloss: 0.36901\tvalid_1's binary_logloss: 0.384107\n",
      "[1600]\ttraining's binary_logloss: 0.365944\tvalid_1's binary_logloss: 0.382106\n",
      "[1700]\ttraining's binary_logloss: 0.36334\tvalid_1's binary_logloss: 0.380659\n",
      "[1800]\ttraining's binary_logloss: 0.360812\tvalid_1's binary_logloss: 0.379279\n",
      "[1900]\ttraining's binary_logloss: 0.358241\tvalid_1's binary_logloss: 0.37786\n",
      "[2000]\ttraining's binary_logloss: 0.355705\tvalid_1's binary_logloss: 0.376497\n",
      "[2100]\ttraining's binary_logloss: 0.353488\tvalid_1's binary_logloss: 0.375407\n",
      "[2200]\ttraining's binary_logloss: 0.351275\tvalid_1's binary_logloss: 0.374328\n",
      "[2300]\ttraining's binary_logloss: 0.348858\tvalid_1's binary_logloss: 0.373022\n",
      "[2400]\ttraining's binary_logloss: 0.346838\tvalid_1's binary_logloss: 0.372075\n",
      "[2500]\ttraining's binary_logloss: 0.344346\tvalid_1's binary_logloss: 0.37068\n",
      "[2600]\ttraining's binary_logloss: 0.342239\tvalid_1's binary_logloss: 0.369671\n",
      "[2700]\ttraining's binary_logloss: 0.340044\tvalid_1's binary_logloss: 0.368554\n",
      "[2800]\ttraining's binary_logloss: 0.338042\tvalid_1's binary_logloss: 0.367653\n",
      "[2900]\ttraining's binary_logloss: 0.33586\tvalid_1's binary_logloss: 0.3666\n",
      "[3000]\ttraining's binary_logloss: 0.333952\tvalid_1's binary_logloss: 0.365751\n",
      "[3100]\ttraining's binary_logloss: 0.331956\tvalid_1's binary_logloss: 0.36483\n",
      "[3200]\ttraining's binary_logloss: 0.329529\tvalid_1's binary_logloss: 0.363493\n",
      "[3300]\ttraining's binary_logloss: 0.32745\tvalid_1's binary_logloss: 0.362467\n",
      "[3400]\ttraining's binary_logloss: 0.325342\tvalid_1's binary_logloss: 0.361393\n",
      "[3500]\ttraining's binary_logloss: 0.323551\tvalid_1's binary_logloss: 0.360614\n",
      "[3600]\ttraining's binary_logloss: 0.321485\tvalid_1's binary_logloss: 0.359589\n",
      "[3700]\ttraining's binary_logloss: 0.319559\tvalid_1's binary_logloss: 0.358681\n",
      "[3800]\ttraining's binary_logloss: 0.317691\tvalid_1's binary_logloss: 0.357822\n",
      "[3900]\ttraining's binary_logloss: 0.31581\tvalid_1's binary_logloss: 0.356976\n",
      "[4000]\ttraining's binary_logloss: 0.313955\tvalid_1's binary_logloss: 0.35615\n",
      "[4100]\ttraining's binary_logloss: 0.311697\tvalid_1's binary_logloss: 0.354864\n",
      "[4200]\ttraining's binary_logloss: 0.309926\tvalid_1's binary_logloss: 0.354118\n",
      "[4300]\ttraining's binary_logloss: 0.30854\tvalid_1's binary_logloss: 0.353761\n",
      "[4400]\ttraining's binary_logloss: 0.30684\tvalid_1's binary_logloss: 0.353025\n",
      "[4500]\ttraining's binary_logloss: 0.304968\tvalid_1's binary_logloss: 0.352137\n",
      "[4600]\ttraining's binary_logloss: 0.303155\tvalid_1's binary_logloss: 0.351282\n",
      "[4700]\ttraining's binary_logloss: 0.301643\tvalid_1's binary_logloss: 0.350727\n",
      "[4800]\ttraining's binary_logloss: 0.29958\tvalid_1's binary_logloss: 0.349641\n",
      "[4900]\ttraining's binary_logloss: 0.297822\tvalid_1's binary_logloss: 0.348832\n",
      "[5000]\ttraining's binary_logloss: 0.296092\tvalid_1's binary_logloss: 0.348014\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's binary_logloss: 0.296092\tvalid_1's binary_logloss: 0.348014\n",
      "Kaggle Metric LGBM validation = 0.7846451985669469 \n",
      "\n",
      "Kaggle Metric LGBM top 4% = 0.6488681309433645 \n",
      " Gini Metric = 0.9204222661905295\n",
      "#########################\n",
      "### Fold 2\n",
      "### Train size 367130 Valid size 91783\n",
      "### Train shape (458913, 2253)\n",
      "### Training with 100% fold data...\n",
      "#########################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.450162\tvalid_1's binary_logloss: 0.449351\n",
      "[200]\ttraining's binary_logloss: 0.423041\tvalid_1's binary_logloss: 0.42238\n",
      "[300]\ttraining's binary_logloss: 0.415305\tvalid_1's binary_logloss: 0.41507\n",
      "[400]\ttraining's binary_logloss: 0.410227\tvalid_1's binary_logloss: 0.410544\n",
      "[500]\ttraining's binary_logloss: 0.405696\tvalid_1's binary_logloss: 0.406636\n",
      "[600]\ttraining's binary_logloss: 0.400963\tvalid_1's binary_logloss: 0.402666\n",
      "[700]\ttraining's binary_logloss: 0.396734\tvalid_1's binary_logloss: 0.399334\n",
      "[800]\ttraining's binary_logloss: 0.392304\tvalid_1's binary_logloss: 0.395873\n",
      "[900]\ttraining's binary_logloss: 0.388305\tvalid_1's binary_logloss: 0.392978\n",
      "[1000]\ttraining's binary_logloss: 0.384624\tvalid_1's binary_logloss: 0.390458\n",
      "[1100]\ttraining's binary_logloss: 0.380901\tvalid_1's binary_logloss: 0.387885\n",
      "[1200]\ttraining's binary_logloss: 0.377348\tvalid_1's binary_logloss: 0.38547\n",
      "[1300]\ttraining's binary_logloss: 0.374412\tvalid_1's binary_logloss: 0.383652\n",
      "[1400]\ttraining's binary_logloss: 0.371813\tvalid_1's binary_logloss: 0.382198\n",
      "[1500]\ttraining's binary_logloss: 0.369029\tvalid_1's binary_logloss: 0.38054\n",
      "[1600]\ttraining's binary_logloss: 0.366136\tvalid_1's binary_logloss: 0.37878\n",
      "[1700]\ttraining's binary_logloss: 0.363601\tvalid_1's binary_logloss: 0.377347\n",
      "[1800]\ttraining's binary_logloss: 0.361028\tvalid_1's binary_logloss: 0.375941\n",
      "[1900]\ttraining's binary_logloss: 0.358658\tvalid_1's binary_logloss: 0.37473\n",
      "[2000]\ttraining's binary_logloss: 0.356257\tvalid_1's binary_logloss: 0.373518\n",
      "[2100]\ttraining's binary_logloss: 0.353971\tvalid_1's binary_logloss: 0.37235\n",
      "[2200]\ttraining's binary_logloss: 0.351539\tvalid_1's binary_logloss: 0.371006\n",
      "[2300]\ttraining's binary_logloss: 0.349045\tvalid_1's binary_logloss: 0.369639\n",
      "[2400]\ttraining's binary_logloss: 0.347\tvalid_1's binary_logloss: 0.368719\n",
      "[2500]\ttraining's binary_logloss: 0.344576\tvalid_1's binary_logloss: 0.367372\n",
      "[2600]\ttraining's binary_logloss: 0.34253\tvalid_1's binary_logloss: 0.366426\n",
      "[2700]\ttraining's binary_logloss: 0.340327\tvalid_1's binary_logloss: 0.365292\n",
      "[2800]\ttraining's binary_logloss: 0.338503\tvalid_1's binary_logloss: 0.364557\n",
      "[2900]\ttraining's binary_logloss: 0.3362\tvalid_1's binary_logloss: 0.363361\n",
      "[3000]\ttraining's binary_logloss: 0.334102\tvalid_1's binary_logloss: 0.362368\n",
      "[3100]\ttraining's binary_logloss: 0.3321\tvalid_1's binary_logloss: 0.361453\n",
      "[3200]\ttraining's binary_logloss: 0.329957\tvalid_1's binary_logloss: 0.360359\n",
      "[3300]\ttraining's binary_logloss: 0.328227\tvalid_1's binary_logloss: 0.359678\n",
      "[3400]\ttraining's binary_logloss: 0.325818\tvalid_1's binary_logloss: 0.358345\n",
      "[3500]\ttraining's binary_logloss: 0.323664\tvalid_1's binary_logloss: 0.357215\n",
      "[3600]\ttraining's binary_logloss: 0.322087\tvalid_1's binary_logloss: 0.356654\n",
      "[3700]\ttraining's binary_logloss: 0.319995\tvalid_1's binary_logloss: 0.355615\n",
      "[3800]\ttraining's binary_logloss: 0.318094\tvalid_1's binary_logloss: 0.354741\n",
      "[3900]\ttraining's binary_logloss: 0.316135\tvalid_1's binary_logloss: 0.353792\n",
      "[4000]\ttraining's binary_logloss: 0.314497\tvalid_1's binary_logloss: 0.353149\n",
      "[4100]\ttraining's binary_logloss: 0.312646\tvalid_1's binary_logloss: 0.352309\n",
      "[4200]\ttraining's binary_logloss: 0.310938\tvalid_1's binary_logloss: 0.351637\n",
      "[4300]\ttraining's binary_logloss: 0.308863\tvalid_1's binary_logloss: 0.350532\n",
      "[4400]\ttraining's binary_logloss: 0.30694\tvalid_1's binary_logloss: 0.349593\n",
      "[4500]\ttraining's binary_logloss: 0.305166\tvalid_1's binary_logloss: 0.348792\n",
      "[4600]\ttraining's binary_logloss: 0.303682\tvalid_1's binary_logloss: 0.348325\n",
      "[4700]\ttraining's binary_logloss: 0.301979\tvalid_1's binary_logloss: 0.347601\n",
      "[4800]\ttraining's binary_logloss: 0.300074\tvalid_1's binary_logloss: 0.346698\n",
      "[4900]\ttraining's binary_logloss: 0.298528\tvalid_1's binary_logloss: 0.346145\n",
      "[5000]\ttraining's binary_logloss: 0.296572\tvalid_1's binary_logloss: 0.345186\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's binary_logloss: 0.296572\tvalid_1's binary_logloss: 0.345186\n",
      "Kaggle Metric LGBM validation = 0.7853476430804528 \n",
      "\n",
      "Kaggle Metric LGBM top 4% = 0.6493309770260035 \n",
      " Gini Metric = 0.9213643091349021\n",
      "#########################\n",
      "### Fold 3\n",
      "### Train size 367130 Valid size 91783\n",
      "### Train shape (458913, 2253)\n",
      "### Training with 100% fold data...\n",
      "#########################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.448335\tvalid_1's binary_logloss: 0.451108\n",
      "[200]\ttraining's binary_logloss: 0.421356\tvalid_1's binary_logloss: 0.426192\n",
      "[300]\ttraining's binary_logloss: 0.413271\tvalid_1's binary_logloss: 0.419835\n",
      "[400]\ttraining's binary_logloss: 0.409195\tvalid_1's binary_logloss: 0.417182\n",
      "[500]\ttraining's binary_logloss: 0.404404\tvalid_1's binary_logloss: 0.413546\n",
      "[600]\ttraining's binary_logloss: 0.399493\tvalid_1's binary_logloss: 0.409779\n",
      "[700]\ttraining's binary_logloss: 0.395054\tvalid_1's binary_logloss: 0.406421\n",
      "[800]\ttraining's binary_logloss: 0.390482\tvalid_1's binary_logloss: 0.402984\n",
      "[900]\ttraining's binary_logloss: 0.386479\tvalid_1's binary_logloss: 0.400115\n",
      "[1000]\ttraining's binary_logloss: 0.382759\tvalid_1's binary_logloss: 0.397513\n",
      "[1100]\ttraining's binary_logloss: 0.379376\tvalid_1's binary_logloss: 0.395247\n",
      "[1200]\ttraining's binary_logloss: 0.376009\tvalid_1's binary_logloss: 0.393047\n",
      "[1300]\ttraining's binary_logloss: 0.373013\tvalid_1's binary_logloss: 0.39123\n",
      "[1400]\ttraining's binary_logloss: 0.370084\tvalid_1's binary_logloss: 0.389438\n",
      "[1500]\ttraining's binary_logloss: 0.367267\tvalid_1's binary_logloss: 0.387796\n",
      "[1600]\ttraining's binary_logloss: 0.364517\tvalid_1's binary_logloss: 0.386186\n",
      "[1700]\ttraining's binary_logloss: 0.362363\tvalid_1's binary_logloss: 0.385157\n",
      "[1800]\ttraining's binary_logloss: 0.359687\tvalid_1's binary_logloss: 0.383662\n",
      "[1900]\ttraining's binary_logloss: 0.357384\tvalid_1's binary_logloss: 0.382475\n",
      "[2000]\ttraining's binary_logloss: 0.354753\tvalid_1's binary_logloss: 0.380958\n",
      "[2100]\ttraining's binary_logloss: 0.352189\tvalid_1's binary_logloss: 0.379537\n",
      "[2200]\ttraining's binary_logloss: 0.349837\tvalid_1's binary_logloss: 0.378299\n",
      "[2300]\ttraining's binary_logloss: 0.347408\tvalid_1's binary_logloss: 0.37697\n",
      "[2400]\ttraining's binary_logloss: 0.345016\tvalid_1's binary_logloss: 0.375674\n",
      "[2500]\ttraining's binary_logloss: 0.342709\tvalid_1's binary_logloss: 0.374491\n",
      "[2600]\ttraining's binary_logloss: 0.341036\tvalid_1's binary_logloss: 0.373902\n",
      "[2700]\ttraining's binary_logloss: 0.338818\tvalid_1's binary_logloss: 0.37276\n",
      "[2800]\ttraining's binary_logloss: 0.33669\tvalid_1's binary_logloss: 0.371721\n",
      "[2900]\ttraining's binary_logloss: 0.334633\tvalid_1's binary_logloss: 0.370708\n",
      "[3000]\ttraining's binary_logloss: 0.332552\tvalid_1's binary_logloss: 0.369696\n",
      "[3100]\ttraining's binary_logloss: 0.330538\tvalid_1's binary_logloss: 0.368739\n",
      "[3200]\ttraining's binary_logloss: 0.328328\tvalid_1's binary_logloss: 0.367586\n",
      "[3300]\ttraining's binary_logloss: 0.326422\tvalid_1's binary_logloss: 0.366732\n",
      "[3400]\ttraining's binary_logloss: 0.324426\tvalid_1's binary_logloss: 0.365809\n",
      "[3500]\ttraining's binary_logloss: 0.322699\tvalid_1's binary_logloss: 0.365117\n",
      "[3600]\ttraining's binary_logloss: 0.320666\tvalid_1's binary_logloss: 0.364099\n",
      "[3700]\ttraining's binary_logloss: 0.318783\tvalid_1's binary_logloss: 0.363276\n",
      "[3800]\ttraining's binary_logloss: 0.31659\tvalid_1's binary_logloss: 0.362104\n",
      "[3900]\ttraining's binary_logloss: 0.314704\tvalid_1's binary_logloss: 0.361246\n",
      "[4000]\ttraining's binary_logloss: 0.312835\tvalid_1's binary_logloss: 0.36042\n",
      "[4100]\ttraining's binary_logloss: 0.311237\tvalid_1's binary_logloss: 0.359837\n",
      "[4200]\ttraining's binary_logloss: 0.309257\tvalid_1's binary_logloss: 0.358867\n",
      "[4300]\ttraining's binary_logloss: 0.307127\tvalid_1's binary_logloss: 0.357784\n",
      "[4400]\ttraining's binary_logloss: 0.30565\tvalid_1's binary_logloss: 0.357301\n",
      "[4500]\ttraining's binary_logloss: 0.303829\tvalid_1's binary_logloss: 0.356443\n",
      "[4600]\ttraining's binary_logloss: 0.301729\tvalid_1's binary_logloss: 0.355281\n",
      "[4700]\ttraining's binary_logloss: 0.299983\tvalid_1's binary_logloss: 0.354551\n",
      "[4800]\ttraining's binary_logloss: 0.298194\tvalid_1's binary_logloss: 0.353762\n",
      "[4900]\ttraining's binary_logloss: 0.296807\tvalid_1's binary_logloss: 0.353294\n",
      "[5000]\ttraining's binary_logloss: 0.295083\tvalid_1's binary_logloss: 0.352527\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's binary_logloss: 0.295083\tvalid_1's binary_logloss: 0.352527\n",
      "Kaggle Metric LGBM validation = 0.7835533619823005 \n",
      "\n",
      "Kaggle Metric LGBM top 4% = 0.6489943616931751 \n",
      " Gini Metric = 0.9181123622714258\n",
      "#########################\n",
      "### Fold 4\n",
      "### Train size 367131 Valid size 91782\n",
      "### Train shape (458913, 2253)\n",
      "### Training with 100% fold data...\n",
      "#########################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.450068\tvalid_1's binary_logloss: 0.449773\n",
      "[200]\ttraining's binary_logloss: 0.422599\tvalid_1's binary_logloss: 0.422238\n",
      "[300]\ttraining's binary_logloss: 0.415248\tvalid_1's binary_logloss: 0.415182\n",
      "[400]\ttraining's binary_logloss: 0.410819\tvalid_1's binary_logloss: 0.411424\n",
      "[500]\ttraining's binary_logloss: 0.405923\tvalid_1's binary_logloss: 0.407298\n",
      "[600]\ttraining's binary_logloss: 0.401901\tvalid_1's binary_logloss: 0.404192\n",
      "[700]\ttraining's binary_logloss: 0.397098\tvalid_1's binary_logloss: 0.400428\n",
      "[800]\ttraining's binary_logloss: 0.392248\tvalid_1's binary_logloss: 0.396755\n",
      "[900]\ttraining's binary_logloss: 0.387996\tvalid_1's binary_logloss: 0.393716\n",
      "[1000]\ttraining's binary_logloss: 0.38435\tvalid_1's binary_logloss: 0.391293\n",
      "[1100]\ttraining's binary_logloss: 0.380774\tvalid_1's binary_logloss: 0.388915\n",
      "[1200]\ttraining's binary_logloss: 0.377517\tvalid_1's binary_logloss: 0.38687\n",
      "[1300]\ttraining's binary_logloss: 0.374605\tvalid_1's binary_logloss: 0.385186\n",
      "[1400]\ttraining's binary_logloss: 0.371736\tvalid_1's binary_logloss: 0.383484\n",
      "[1500]\ttraining's binary_logloss: 0.369205\tvalid_1's binary_logloss: 0.382125\n",
      "[1600]\ttraining's binary_logloss: 0.3664\tvalid_1's binary_logloss: 0.380505\n",
      "[1700]\ttraining's binary_logloss: 0.363768\tvalid_1's binary_logloss: 0.379101\n",
      "[1800]\ttraining's binary_logloss: 0.361107\tvalid_1's binary_logloss: 0.377628\n",
      "[1900]\ttraining's binary_logloss: 0.35873\tvalid_1's binary_logloss: 0.376426\n",
      "[2000]\ttraining's binary_logloss: 0.35613\tvalid_1's binary_logloss: 0.375013\n",
      "[2100]\ttraining's binary_logloss: 0.35397\tvalid_1's binary_logloss: 0.374034\n",
      "[2200]\ttraining's binary_logloss: 0.351486\tvalid_1's binary_logloss: 0.372681\n",
      "[2300]\ttraining's binary_logloss: 0.349223\tvalid_1's binary_logloss: 0.37154\n",
      "[2400]\ttraining's binary_logloss: 0.346766\tvalid_1's binary_logloss: 0.370227\n",
      "[2500]\ttraining's binary_logloss: 0.344243\tvalid_1's binary_logloss: 0.368835\n",
      "[2600]\ttraining's binary_logloss: 0.34207\tvalid_1's binary_logloss: 0.367754\n",
      "[2700]\ttraining's binary_logloss: 0.339887\tvalid_1's binary_logloss: 0.366655\n",
      "[2800]\ttraining's binary_logloss: 0.338037\tvalid_1's binary_logloss: 0.365927\n",
      "[2900]\ttraining's binary_logloss: 0.335991\tvalid_1's binary_logloss: 0.364968\n",
      "[3000]\ttraining's binary_logloss: 0.333767\tvalid_1's binary_logloss: 0.363843\n",
      "[3100]\ttraining's binary_logloss: 0.331739\tvalid_1's binary_logloss: 0.362869\n",
      "[3200]\ttraining's binary_logloss: 0.329627\tvalid_1's binary_logloss: 0.361817\n",
      "[3300]\ttraining's binary_logloss: 0.327559\tvalid_1's binary_logloss: 0.360792\n",
      "[3400]\ttraining's binary_logloss: 0.325522\tvalid_1's binary_logloss: 0.35982\n",
      "[3500]\ttraining's binary_logloss: 0.323369\tvalid_1's binary_logloss: 0.358729\n",
      "[3600]\ttraining's binary_logloss: 0.321446\tvalid_1's binary_logloss: 0.357817\n",
      "[3700]\ttraining's binary_logloss: 0.319593\tvalid_1's binary_logloss: 0.357052\n",
      "[3800]\ttraining's binary_logloss: 0.317824\tvalid_1's binary_logloss: 0.356329\n",
      "[3900]\ttraining's binary_logloss: 0.315949\tvalid_1's binary_logloss: 0.355498\n",
      "[4000]\ttraining's binary_logloss: 0.31396\tvalid_1's binary_logloss: 0.354531\n",
      "[4100]\ttraining's binary_logloss: 0.311964\tvalid_1's binary_logloss: 0.353558\n",
      "[4200]\ttraining's binary_logloss: 0.310129\tvalid_1's binary_logloss: 0.352757\n",
      "[4300]\ttraining's binary_logloss: 0.30837\tvalid_1's binary_logloss: 0.352023\n",
      "[4400]\ttraining's binary_logloss: 0.306233\tvalid_1's binary_logloss: 0.350895\n",
      "[4500]\ttraining's binary_logloss: 0.304746\tvalid_1's binary_logloss: 0.350414\n",
      "[4600]\ttraining's binary_logloss: 0.302828\tvalid_1's binary_logloss: 0.349494\n",
      "[4700]\ttraining's binary_logloss: 0.301033\tvalid_1's binary_logloss: 0.34867\n",
      "[4800]\ttraining's binary_logloss: 0.299278\tvalid_1's binary_logloss: 0.347903\n",
      "[4900]\ttraining's binary_logloss: 0.297879\tvalid_1's binary_logloss: 0.347497\n",
      "[5000]\ttraining's binary_logloss: 0.295865\tvalid_1's binary_logloss: 0.346504\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's binary_logloss: 0.295865\tvalid_1's binary_logloss: 0.346504\n",
      "Kaggle Metric LGBM validation = 0.7856042137017931 \n",
      "\n",
      "Kaggle Metric LGBM top 4% = 0.6506206606353881 \n",
      " Gini Metric = 0.9205877667681982\n",
      "#########################\n",
      "### Fold 5\n",
      "### Train size 367131 Valid size 91782\n",
      "### Train shape (458913, 2253)\n",
      "### Training with 100% fold data...\n",
      "#########################\n",
      "Training until validation scores don't improve for 2000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.449695\tvalid_1's binary_logloss: 0.449842\n",
      "[200]\ttraining's binary_logloss: 0.422994\tvalid_1's binary_logloss: 0.422933\n",
      "[300]\ttraining's binary_logloss: 0.415159\tvalid_1's binary_logloss: 0.415095\n",
      "[400]\ttraining's binary_logloss: 0.410054\tvalid_1's binary_logloss: 0.410275\n",
      "[500]\ttraining's binary_logloss: 0.405935\tvalid_1's binary_logloss: 0.406608\n",
      "[600]\ttraining's binary_logloss: 0.401311\tvalid_1's binary_logloss: 0.402672\n",
      "[700]\ttraining's binary_logloss: 0.397087\tvalid_1's binary_logloss: 0.399202\n",
      "[800]\ttraining's binary_logloss: 0.392462\tvalid_1's binary_logloss: 0.395459\n",
      "[900]\ttraining's binary_logloss: 0.388523\tvalid_1's binary_logloss: 0.392474\n",
      "[1000]\ttraining's binary_logloss: 0.384567\tvalid_1's binary_logloss: 0.38955\n",
      "[1100]\ttraining's binary_logloss: 0.38107\tvalid_1's binary_logloss: 0.387063\n",
      "[1200]\ttraining's binary_logloss: 0.377764\tvalid_1's binary_logloss: 0.384789\n",
      "[1300]\ttraining's binary_logloss: 0.374766\tvalid_1's binary_logloss: 0.382891\n",
      "[1400]\ttraining's binary_logloss: 0.372628\tvalid_1's binary_logloss: 0.381816\n",
      "[1500]\ttraining's binary_logloss: 0.369364\tvalid_1's binary_logloss: 0.379649\n",
      "[1600]\ttraining's binary_logloss: 0.366889\tvalid_1's binary_logloss: 0.378298\n",
      "[1700]\ttraining's binary_logloss: 0.364364\tvalid_1's binary_logloss: 0.376837\n",
      "[1800]\ttraining's binary_logloss: 0.362014\tvalid_1's binary_logloss: 0.375648\n",
      "[1900]\ttraining's binary_logloss: 0.35921\tvalid_1's binary_logloss: 0.373927\n",
      "[2000]\ttraining's binary_logloss: 0.356944\tvalid_1's binary_logloss: 0.372713\n",
      "[2100]\ttraining's binary_logloss: 0.354261\tvalid_1's binary_logloss: 0.371161\n",
      "[2200]\ttraining's binary_logloss: 0.352108\tvalid_1's binary_logloss: 0.370076\n",
      "[2300]\ttraining's binary_logloss: 0.349502\tvalid_1's binary_logloss: 0.368573\n",
      "[2400]\ttraining's binary_logloss: 0.347409\tvalid_1's binary_logloss: 0.367584\n",
      "[2500]\ttraining's binary_logloss: 0.345079\tvalid_1's binary_logloss: 0.366373\n",
      "[2600]\ttraining's binary_logloss: 0.342797\tvalid_1's binary_logloss: 0.365192\n",
      "[2700]\ttraining's binary_logloss: 0.340629\tvalid_1's binary_logloss: 0.364093\n",
      "[2800]\ttraining's binary_logloss: 0.338316\tvalid_1's binary_logloss: 0.362864\n",
      "[2900]\ttraining's binary_logloss: 0.33616\tvalid_1's binary_logloss: 0.361799\n",
      "[3000]\ttraining's binary_logloss: 0.334385\tvalid_1's binary_logloss: 0.361081\n",
      "[3100]\ttraining's binary_logloss: 0.332442\tvalid_1's binary_logloss: 0.360198\n",
      "[3200]\ttraining's binary_logloss: 0.330531\tvalid_1's binary_logloss: 0.359385\n",
      "[3300]\ttraining's binary_logloss: 0.328067\tvalid_1's binary_logloss: 0.357954\n",
      "[3400]\ttraining's binary_logloss: 0.32632\tvalid_1's binary_logloss: 0.357214\n",
      "[3500]\ttraining's binary_logloss: 0.324111\tvalid_1's binary_logloss: 0.356018\n",
      "[3600]\ttraining's binary_logloss: 0.322301\tvalid_1's binary_logloss: 0.355233\n",
      "[3700]\ttraining's binary_logloss: 0.32032\tvalid_1's binary_logloss: 0.354246\n",
      "[3800]\ttraining's binary_logloss: 0.31844\tvalid_1's binary_logloss: 0.353364\n",
      "[3900]\ttraining's binary_logloss: 0.316471\tvalid_1's binary_logloss: 0.352371\n",
      "[4000]\ttraining's binary_logloss: 0.314479\tvalid_1's binary_logloss: 0.351391\n",
      "[4100]\ttraining's binary_logloss: 0.31241\tvalid_1's binary_logloss: 0.350312\n",
      "[4200]\ttraining's binary_logloss: 0.310653\tvalid_1's binary_logloss: 0.34955\n",
      "[4300]\ttraining's binary_logloss: 0.308989\tvalid_1's binary_logloss: 0.348905\n",
      "[4400]\ttraining's binary_logloss: 0.307023\tvalid_1's binary_logloss: 0.347954\n",
      "[4500]\ttraining's binary_logloss: 0.305529\tvalid_1's binary_logloss: 0.34743\n",
      "[4600]\ttraining's binary_logloss: 0.303639\tvalid_1's binary_logloss: 0.346472\n",
      "[4700]\ttraining's binary_logloss: 0.302007\tvalid_1's binary_logloss: 0.345771\n",
      "[4800]\ttraining's binary_logloss: 0.300361\tvalid_1's binary_logloss: 0.345117\n",
      "[4900]\ttraining's binary_logloss: 0.298439\tvalid_1's binary_logloss: 0.344178\n",
      "[5000]\ttraining's binary_logloss: 0.296711\tvalid_1's binary_logloss: 0.343398\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's binary_logloss: 0.296711\tvalid_1's binary_logloss: 0.343398\n",
      "Kaggle Metric LGBM validation = 0.790167795702724 \n",
      "\n",
      "Kaggle Metric LGBM top 4% = 0.6585314538186409 \n",
      " Gini Metric = 0.921804137586807\n",
      "#########################\n",
      "OVERALL CV Kaggle Metric LightGBm= 0.7858407694288914\n"
     ]
    }
   ],
   "source": [
    "############################## LightGBM #####################\n",
    "\n",
    "importances = []\n",
    "permutation_importances = []\n",
    "oof = []\n",
    "\n",
    "TRAIN_SUBSAMPLE = 1.0\n",
    "FEATURES = train.columns[1:-1]\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=Model_Parameters.FOLDS, shuffle=True, random_state=Model_Parameters.SEED)\n",
    "\n",
    "if Train_Parameters.train_hard_samples:\n",
    "    target = 'hard_indicator'\n",
    "else:        \n",
    "    target = 'target'\n",
    "\n",
    "\n",
    "for fold,(train_idx, valid_idx) in enumerate(skf.split(\n",
    "            train, train[f'{target}'])):\n",
    "\n",
    "    # TRAIN WITH SUBSAMPLE OF TRAIN FOLD DATA\n",
    "    if TRAIN_SUBSAMPLE<1.0:\n",
    "        np.random.seed(SEED)\n",
    "        train_idx = np.random.choice(train_idx, \n",
    "                       int(len(train_idx)*TRAIN_SUBSAMPLE), replace=False)\n",
    "        np.random.seed(None)\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### Fold',fold+1)\n",
    "    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n",
    "    print('### Train shape',train.loc[:, FEATURES].shape)\n",
    "    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n",
    "    print('#'*25)\n",
    "\n",
    "    X_valid = train.loc[valid_idx, FEATURES]\n",
    "    X_train = train.loc[train_idx, FEATURES]\n",
    "    Y_valid = train.loc[valid_idx, f'{target}']\n",
    "    Y_train = train.loc[train_idx, f'{target}']\n",
    "\n",
    "    ###### LGBM Model #######\n",
    "    # lgb_model, oof_preds = train_lgb_model(X_valid, Y_valid, X_train, Y_train, params=xgb_parms, num_rounds=100)#Train_Parameters.training_rounds)    \n",
    "    lgb_model, oof_preds = train_lgb_model(X_valid, Y_valid, X_train, Y_train, num_rounds = 5000) #Train_Parameters.training_rounds)    \n",
    "    lgb_model.save_model(f'Model_Weights/LGBM_v{Model_Parameters.VER}_fold{fold}.json')\n",
    "\n",
    "    # GET FEATURE IMPORTANCE FOR FOLD K\n",
    "    # dd = model.get_score(importance_type='weight')\n",
    "    # df = pd.DataFrame({'feature':dd.keys(),f'importance_{fold}':dd.values()})\n",
    "    # importances.append(df)\n",
    "\n",
    "    # INFER OOF FOLD K\n",
    "    acc = amex_metric_mod(Y_valid.values, oof_preds)\n",
    "    print('Kaggle Metric LGBM validation =',acc,'\\n')\n",
    "\n",
    "    acc = top_four_percent(Y_valid.values, lgb_model.predict(X_valid))\n",
    "    acc2 = gini_metric(Y_valid.values, lgb_model.predict(X_valid))\n",
    "    \n",
    "    print('Kaggle Metric LGBM top 4% =',acc,'\\n', 'Gini Metric =', acc2)\n",
    "\n",
    "    # SAVE OOF\n",
    "    df = train.loc[valid_idx, ['customer_ID',f'{target}'] ].copy()\n",
    "    df['oof_pred'] = oof_preds\n",
    "    oof.append( df )\n",
    "\n",
    "    del X_valid, Y_valid, X_train, Y_train, lgb_model\n",
    "    _ = gc.collect()\n",
    "\n",
    "print('#'*25)\n",
    "oof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\n",
    "acc = amex_metric_mod(oof[f'{target}'], oof.oof_pred.values)\n",
    "print('OVERALL CV Kaggle Metric LightGBm=',acc)\n",
    "\n",
    "oof_lgb = train_df.drop_duplicates(subset=['customer_ID'])\n",
    "\n",
    "oof_lgb = oof_lgb.loc[:,'customer_ID']\n",
    "oof_lgb = oof_lgb.to_frame()\n",
    "\n",
    "oof_lgb = oof_lgb.merge(oof, left_on='customer_ID', right_on='customer_ID')\n",
    "\n",
    "oof_lgb.to_csv(f'oof_lgb_v{Model_Parameters.VER}.csv',index=False)\n",
    "oof_lgb.head()\n",
    "del oof_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b171f-eacb-437f-8a0a-d3b4e891a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kaggle Metric LGBM top 4% = 0.7188571669016011 \n",
    " Gini Metric = 0.9392231327180331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922a265c-a13e-4635-b210-9e799311bac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "### Train size 367130 Valid size 91783\n",
      "### Train shape (458913, 2253)\n",
      "### Training with 100% fold data...\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/data.py:323: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "/home/julian/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/data.py:317: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[12:46:07] ../src/data/data.cc:1163: Check failed: valid: Input data contains `inf` or `nan`\nStack trace:\n  [bt] (0) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x154e19) [0x7f59bcdb3e19]\n  [bt] (1) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x179fbd) [0x7f59bcdd8fbd]\n  [bt] (2) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1ab08a) [0x7f59bce0a08a]\n  [bt] (3) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x16a475) [0x7f59bcdc9475]\n  [bt] (4) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGDMatrixCreateFromDense+0x453) [0x7f59bcd13ae3]\n  [bt] (5) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7f5a3e468ec0]\n  [bt] (6) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7f5a3e46887d]\n  [bt] (7) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f5a3f435ede]\n  [bt] (8) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12914) [0x7f5a3f436914]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8e404e511cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{target}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_xgb_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_parms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrain_Parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_rounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Model_Weights/XGB_v{Model_Parameters.VER}_fold{fold}.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ddca387c3257>\u001b[0m in \u001b[0;36mtrain_xgb_model\u001b[0;34m(X_valid, Y_valid, X_train, Y_train, params, num_rounds, existing_model)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_xgb_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mfeature_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m             \u001b[0menable_categorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_categorical\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         )\n\u001b[1;32m    651\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_pandas_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         return _from_pandas_df(data, enable_categorical, missing, threads,\n\u001b[0;32m--> 897\u001b[0;31m                                feature_names, feature_types)\n\u001b[0m\u001b[1;32m    898\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_pandas_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         return _from_pandas_series(\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_from_pandas_df\u001b[0;34m(data, enable_categorical, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     )\n\u001b[0;32m--> 348\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_from_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_from_numpy_array\u001b[0;34m(data, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0m_array_interface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         )\n\u001b[1;32m    190\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \"\"\"\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [12:46:07] ../src/data/data.cc:1163: Check failed: valid: Input data contains `inf` or `nan`\nStack trace:\n  [bt] (0) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x154e19) [0x7f59bcdb3e19]\n  [bt] (1) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x179fbd) [0x7f59bcdd8fbd]\n  [bt] (2) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1ab08a) [0x7f59bce0a08a]\n  [bt] (3) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x16a475) [0x7f59bcdc9475]\n  [bt] (4) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGDMatrixCreateFromDense+0x453) [0x7f59bcd13ae3]\n  [bt] (5) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7f5a3e468ec0]\n  [bt] (6) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7f5a3e46887d]\n  [bt] (7) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f5a3f435ede]\n  [bt] (8) /home/julian/anaconda3/envs/tf-gpu/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12914) [0x7f5a3f436914]\n\n"
     ]
    }
   ],
   "source": [
    "############################## XGBoost ########\n",
    "\n",
    "importances = []\n",
    "permutation_importances = []\n",
    "oof = []\n",
    "\n",
    "TRAIN_SUBSAMPLE = 1.0\n",
    "FEATURES = train.columns[1:-1]\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=Model_Parameters.FOLDS, shuffle=True, random_state=Model_Parameters.SEED)\n",
    "\n",
    "if Train_Parameters.train_hard_samples:\n",
    "    target = 'hard_indicator'\n",
    "else:        \n",
    "    target = 'target'\n",
    "\n",
    "\n",
    "for fold,(train_idx, valid_idx) in enumerate(skf.split(\n",
    "            train, train[f'{target}'])):\n",
    "\n",
    "    # TRAIN WITH SUBSAMPLE OF TRAIN FOLD DATA\n",
    "    if TRAIN_SUBSAMPLE<1.0:\n",
    "        np.random.seed(SEED)\n",
    "        train_idx = np.random.choice(train_idx, \n",
    "                       int(len(train_idx)*TRAIN_SUBSAMPLE), replace=False)\n",
    "        np.random.seed(None)\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### Fold',fold+1)\n",
    "    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n",
    "    print('### Train shape',train.loc[:, FEATURES].shape)\n",
    "    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n",
    "    print('#'*25)\n",
    "\n",
    "    X_valid = train.loc[valid_idx, FEATURES]\n",
    "    X_train = train.loc[train_idx, FEATURES]\n",
    "    Y_valid = train.loc[valid_idx, f'{target}']\n",
    "    Y_train = train.loc[train_idx, f'{target}']\n",
    "\n",
    "    xgb_model, oof_preds = train_xgb_model(X_valid, Y_valid, X_train, Y_train, params=xgb_parms, num_rounds = Train_Parameters.training_rounds)    \n",
    "    xgb_model.save_model(f'Model_Weights/XGB_v{Model_Parameters.VER}_fold{fold}.json')\n",
    "\n",
    "    # GET FEATURE IMPORTANCE FOR FOLD K\n",
    "    # dd = model.get_score(importance_type='weight')\n",
    "    # df = pd.DataFrame({'feature':dd.keys(),f'importance_{fold}':dd.values()})\n",
    "    # importances.append(df)\n",
    "\n",
    "    # INFER OOF FOLD K\n",
    "    acc = amex_metric_mod(Y_valid.values, oof_preds)\n",
    "    print('Kaggle Metric XGBoost =',acc,'\\n')\n",
    "    \n",
    "    acc = amex_metric_mod(Y_train.values, lgb_model.predict(X_train))\n",
    "    print('Kaggle Metric LGBM validation =',acc,'\\n')\n",
    "\n",
    "    # SAVE OOF\n",
    "    df = train.loc[valid_idx, ['customer_ID',f'{target}'] ].copy()\n",
    "    df['oof_pred'] = oof_preds\n",
    "    oof.append( df )\n",
    "\n",
    "    del X_valid, Y_valid, X_train, Y_train, xgb_model\n",
    "    _ = gc.collect()\n",
    "\n",
    "print('#'*25)\n",
    "oof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\n",
    "acc = amex_metric_mod(oof[f'{target}'], oof.oof_pred.values)\n",
    "print('OVERALL CV Kaggle Metric XGBoost =',acc)\n",
    "\n",
    "oof_xgb = train_df.drop_duplicates(subset=['customer_ID'])\n",
    "\n",
    "oof_xgb = oof_xgb.loc[:,'customer_ID']\n",
    "oof_xgb = oof_xgb.to_frame()\n",
    "\n",
    "oof_xgb = oof_xgb.merge(oof, left_on='customer_ID', right_on='customer_ID')\n",
    "\n",
    "oof_xgb.to_csv(f'oof_xgb_v{Model_Parameters.VER}.csv',index=False)\n",
    "oof_xgb.head()\n",
    "del oof_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7f065-0ec6-4b89-97e8-8cf23d12339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, train_df\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17a75f-74ac-4056-b5fb-0f3b86b436cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optuna section\n",
    "# ##################\n",
    "\n",
    "# def objective(trial):\n",
    "#     param = {\n",
    "#         # 'max_depth':5, \n",
    "#         'max_depth': trial.suggest_int(\n",
    "#             'max_depth',3, 10), \n",
    "#         'learning_rate': trial.suggest_float(\n",
    "#             'learning_rate', 0.001, 0.05, step=0.001), \n",
    "#         'subsample':0.8,\n",
    "#         'colsample_bytree':0.6, \n",
    "#         'eval_metric':'logloss',\n",
    "#         'objective':'binary:logistic',\n",
    "#         'tree_method':'gpu_hist',\n",
    "#         'predictor':'gpu_predictor',\n",
    "#         'random_state':Model_Parameters.SEED,\n",
    "#         'scaled_pos_weight': trial.suggest_int(\n",
    "#             'scaled_pos_weight', 1, 5, 1),\n",
    "#     }\n",
    "    \n",
    "#     FEATURES = train.columns[1:-1]\n",
    "\n",
    "#     X_valid = train.loc[len(train)*0.8:, FEATURES]\n",
    "#     y_valid = train.loc[len(train)*0.8:, 'target']\n",
    "\n",
    "#     dtrain = xgb.DMatrix(data=train.loc[:len(train)*0.8, FEATURES], label=train.loc[:len(train)*0.8, 'target'])\n",
    "#     dvalid = xgb.DMatrix(data=X_valid, label=y_valid)\n",
    "    \n",
    "#     model = xgb.train(param, \n",
    "#                 dtrain=dtrain,\n",
    "#                 evals=[(dtrain,'train'),(dvalid,'valid')],\n",
    "#                 num_boost_round=9999,\n",
    "#                 early_stopping_rounds=100,\n",
    "#                 verbose_eval=100) \n",
    "\n",
    "#     oof_preds = model.predict(dvalid)\n",
    "#     acc = amex_metric_mod(y_valid.values, oof_preds)\n",
    "#     del model\n",
    "#     return acc\n",
    "\n",
    "# # %%time\n",
    "# # study = optuna.create_study(direction='maximize')\n",
    "# # study.optimize(objective, n_trials= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07890956-069f-4b1e-9a08-87a31e282f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PLOT OOF PREDICTIONS\n",
    "# plt.hist(oof_xgb.oof_pred.values, bins=100)\n",
    "# plt.title('OOF Predictions')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3250b4-99dc-47f1-8cb2-794009897953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del oof_xgb, oof\n",
    "# _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be516fd-8fd0-4897-a9a0-91486e8b263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Test\n",
    "test_df = pd.read_parquet('Processed_Data/test.parquet')\n",
    "test_df = pre_process_df(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d9b7c-3607-4efd-860b-8624858cf9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_df.loc[:, ~test_df.columns.isin(Model_Parameters.ignored_columns)]\n",
    "test = agglomeration_function(test, num_features=num_features, cat_features=cat_features, ignore=None, apply_pca=False)\n",
    "customer_ID_cols = test_df.groupby('customer_ID')['customer_ID'].tail(1).reset_index(drop=True)\n",
    "test = pd.concat([customer_ID_cols, test], axis=1)\n",
    "test.to_csv('Processed_Data/test_FE', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db193f9-94f2-4f7f-a3ea-54ba5c68c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.astype(train_dtypes.to_dict())\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc297c8-8229-4133-8b4e-3c1c3ce6e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Train_Parameters.compute_test_df:\n",
    "#     test = test_df.loc[:, ~test_df.columns.isin(Model_Parameters.ignored_columns)]\n",
    "#     test = agglomeration_function(test, num_features=num_features, cat_features=cat_features, ignore=None, apply_pca=False)\n",
    "#     customer_ID_cols = test_df.groupby('customer_ID')['customer_ID'].tail(1).reset_index(drop=True)\n",
    "#     test = pd.concat([customer_ID_cols, test], axis=1)\n",
    "#     test.to_csv('Processed_Data/test_FE', index=False)\n",
    "# else:\n",
    "#     test = pd.read_csv('Processed_Data/test_FE')\n",
    "\n",
    "# # test = test.drop(list(less_important_features.values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c93422-7c06-4557-865d-05d2617b3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main prediction routine for LGB\n",
    "####################################\n",
    "model = lgb.Booster(model_file=f'Model_Weights/LGBM_v{Model_Parameters.VER}_fold0.json')\n",
    "# y_test = xgb.DMatrix(data=test.iloc[:, 1:], enable_categorical=True)\n",
    "y_pred = model.predict(test.iloc[:, 1:])\n",
    "# del model, test\n",
    "\n",
    "for i in range(4):\n",
    "    model = lgb.Booster(model_file=f'Model_Weights/LGBM_v{Model_Parameters.VER}_fold{i+1}.json')\n",
    "    y_pred += model.predict(test.iloc[:, 1:])\n",
    "    del model\n",
    "    \n",
    "y_pred = y_pred/5\n",
    "\n",
    "# create submission\n",
    "sample_df = pd.read_csv('amex-default-prediction/sample_submission.csv')\n",
    "submission = pd.DataFrame({'customer_ID': test['customer_ID'], 'target': y_pred})\n",
    "print(sample_df.shape, submission.shape)\n",
    "sample_df['prediction'] = submission.target.values\n",
    "sample_df.to_csv(f'output_LGBoost_ver{Model_Parameters.VER}.csv', index=False)\n",
    "del sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f668df6f-893f-41ad-9a56-3f555a41a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main prediction routine for XGB\n",
    "####################################\n",
    "test = pd.read_csv('Processed_Data/test_FE')\n",
    "print(test.shape)\n",
    "test = test.iloc[:500000, :]\n",
    "model = xgb.Booster()\n",
    "model.load_model(f'Model_Weights/XGB_v{Model_Parameters.VER}_fold0.json')\n",
    "y_test = xgb.DMatrix(data=test.iloc[:, 1:], enable_categorical=True)\n",
    "y_pred = model.predict(y_test)\n",
    "customer_ID = test['customer_ID']\n",
    "del model, test\n",
    "\n",
    "for i in range(4):\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(f'Model_Weights/XGB_v{Model_Parameters.VER}_fold{i+1}.json')\n",
    "    y_pred += model.predict(y_test)\n",
    "    del model\n",
    "    \n",
    "y_pred = y_pred/5\n",
    "\n",
    "# create submission\n",
    "sample_df = pd.read_csv('amex-default-prediction/sample_submission.csv')\n",
    "sample_df = sample_df.iloc[:500000, :]\n",
    "submission = pd.DataFrame({'customer_ID': customer_ID, 'target': y_pred})\n",
    "print(sample_df.shape, submission.shape)\n",
    "sample_df['prediction'] = submission.target.values\n",
    "sample_df.to_csv(f'output_XGBoost_ver{Model_Parameters.VER}.csv', index=False)\n",
    "del sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77046ec7-6e80-4aa5-b7d6-653fa6170f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main prediction routine for XGB\n",
    "####################################\n",
    "test = pd.read_csv('Processed_Data/test_FE')\n",
    "print(test.shape)\n",
    "test = test.iloc[500000:, :]\n",
    "model = xgb.Booster()\n",
    "model.load_model(f'Model_Weights/XGB_v{Model_Parameters.VER}_fold0.json')\n",
    "y_test = xgb.DMatrix(data=test.iloc[:, 1:], enable_categorical=True)\n",
    "y_pred = model.predict(y_test)\n",
    "customer_ID = test['customer_ID']\n",
    "del model, test\n",
    "\n",
    "for i in range(4):\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(f'Model_Weights/XGB_v{Model_Parameters.VER}_fold{i+1}.json')\n",
    "    y_pred += model.predict(y_test)\n",
    "    del model\n",
    "    \n",
    "y_pred = y_pred/5\n",
    "\n",
    "# create submission\n",
    "sample_df = pd.read_csv('amex-default-prediction/sample_submission.csv')\n",
    "sample_df = sample_df.iloc[500000:, :]\n",
    "submission = pd.DataFrame({'customer_ID': customer_ID, 'target': y_pred})\n",
    "print(sample_df.shape, submission.shape)\n",
    "sample_df['prediction'] = submission.target.values\n",
    "sample_df.to_csv(f'output_XGBoost_ver{Model_Parameters.VER}2nd_half.csv', index=False)\n",
    "del sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbfc852-e366-4e55-84f8-3f88c4f3e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predictions for hard sample\n",
    "# ####################################\n",
    "\n",
    "# model = xgb.Booster()\n",
    "# model.load_model(f'Model_Weights/XGB_hard_indicator_v{Model_Parameters.VER}_fold0.json')\n",
    "# y_test = xgb.DMatrix(data=test.iloc[:, 1:], enable_categorical=True)\n",
    "# y_pred = model.predict(y_test)\n",
    "# del model, test\n",
    "\n",
    "# for i in range(1):\n",
    "#     model = xgb.Booster()\n",
    "#     model.load_model(f'Model_Weights/XGB_v{Model_Parameters.VER}_fold{i+1}.json')\n",
    "#     y_pred += model.predict(y_test)\n",
    "#     del model\n",
    "    \n",
    "# y_pred = y_pred/2\n",
    "# # sample_df = pd.read_csv('amex-default-prediction/sample_submission.csv')\n",
    "# # submission = pd.DataFrame({'customer_ID': test['customer_ID'], 'target': y_pred})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3a9b1-5fab-45e5-8317-7e5d4a4f2679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Secondary Model prediction routine\n",
    "# ####################################\n",
    "\n",
    "# model = xgb.Booster()\n",
    "# model.load_model(f'XGB_v{Model_Parameters.VER}_fold0.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f3c868-db67-4efc-8927-419189b7134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = xgb.DMatrix(data=test.iloc[:, 1:], enable_categorical=True)\n",
    "# y_pred = model.predict(y_test)\n",
    "# del model, test\n",
    "\n",
    "# for i in range(1):\n",
    "#     model = xgb.Booster()\n",
    "#     model.load_model(f'XGB_v{Model_Parameters.VER}_fold{i+1}.json')\n",
    "#     y_pred += model.predict(y_test)\n",
    "#     del model\n",
    "    \n",
    "# y_pred = y_pred/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a7b86f-566f-4421-a47a-8b680d60a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_y_pred = pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0b393-8322-451e-9d98-b99982b7998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_y_pred.to_csv('hard_model_y_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ef439-b6a4-4599-8c04-af00b3e8bc32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5873907-4a24-4738-beca-b8dd3a7a84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create hard sample list\n",
    "# #############\n",
    "\n",
    "# sample_df = pd.read_csv('amex-default-prediction/sample_submission.csv')\n",
    "# submission = pd.DataFrame({'customer_ID': test['customer_ID'], 'target': y_pred})\n",
    "# # submission = submission.groupby(by='customer_ID').mean()\n",
    "# print(sample_df.shape, submission.shape)\n",
    "# sample_df['hard_prediction'] = submission.target.values\n",
    "# sample_df.to_csv(f'output_XGBoost_hard_indicator_ver{Model_Parameters.VER}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8755256c-cb23-456c-acce-7e17f8f44eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f5f043-343d-4250-84ca-9c5bf8f058cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find permutation importance based on test data\n",
    "# model_sklearn_api = xgb.XGBClassifier()\n",
    "# model_sklearn_api.load_model(f'XGB_v{Model_Parameters.VER}_fold{fold}.json')\n",
    "\n",
    "# perm_importance = permutation_importance(model_sklearn_api, test.iloc[:10000,1:], test.iloc[:10000,-1], n_repeats=1, n_jobs=-1)\n",
    "# perm_importance = pd.DataFrame.from_dict(perm_importance.importances_mean)\n",
    "# perm_importance2 = pd.DataFrame({'importances':perm_importance.values.flatten(),'feature name': test.iloc[:, 1:].columns.values})\n",
    "# perm_importance2.to_csv(f'perm_importances_{Model_Parameters.VER}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202014c3-b141-4855-9f11-8c07d7f2fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot a tree for fun\n",
    "\n",
    "# from xgboost import plot_tree\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plot_tree(model, num_trees=1)\n",
    "# fig = plt.gcf()\n",
    "# fig.set_size_inches(300, 150)\n",
    "# plt.savefig('pic.jpg', dpi='figure')\n",
    "\n",
    "# oof_xgb = pd.read_csv(f'oof_xgb_v{Model_Parameters.VER}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d8a32-768e-42fa-9cd1-c434b3421948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Play around with multiple CSVs output\n",
    "# based_pred = pd.read_csv('output_XGBoost_ver2.csv')\n",
    "# hard_indicator = pd.read_csv('output_XGBoost_hard_indicator_ver2.csv')\n",
    "# hard_pred = pd.read_csv('output_XGBoost_hard_indicator_verHARD_Samples_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645ac26-43b7-468d-b152-326d80567a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c602b-bf3a-4b0a-b40e-7800abdd6def",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
